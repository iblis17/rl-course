{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install `Reinforce` Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"Reinforce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi-armed Bandit Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiArmedBandit(10, 1000)  # 10 arms, 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [MultiArmedBandit(10, 1000) for _ ∈ 1:2000];  # generate 2000 k-armds bandit problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your own Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ϵGreedy <: AbstractPolicy\n",
    "    Q::Dict{Int,Float64}\n",
    "    n::Dict{Int,Float64}\n",
    "    ϵ::Float64\n",
    "end\n",
    "ϵGreedy(; ϵ = .1) = ϵGreedy(Dict{Int,Float64}(), Dict{Int,Float64}(), ϵ)\n",
    "\n",
    "function Reinforce.action(π::ϵGreedy, r, s, A)\n",
    "    (rand() < π.ϵ) && return rand(A)\n",
    "\n",
    "    qs = [get(π.Q, a, 0) for a ∈ A]\n",
    "    A[argmax(qs)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function run_single(env; ϵ)\n",
    "    p = ϵGreedy(ϵ = ϵ)\n",
    "    r_seq = Float64[]\n",
    "    run_episode(env, p) do (s, a, r, s′)\n",
    "        n′ = get(p.n, a, 0) + 1\n",
    "        q = get(p.Q, a, 0)\n",
    "        q′ = q + (1 / n′) * (r - q)\n",
    "        # update\n",
    "        p.n[a] = n′\n",
    "        p.Q[a] = q′\n",
    "        # save history\n",
    "        push!(r_seq, r)\n",
    "    end\n",
    "\n",
    "    r_seq\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R₁ = mapreduce(e -> run_single(e, ϵ = 0.1), hcat, envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R₂ = mapreduce(e -> run_single(e, ϵ = 0.01), hcat, envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(R, dims = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(mean(R₁, dims = 2), label = \"epsilon = 0.1\", ylabel = \"average reward\", xlabel = \"steps\")\n",
    "plot!(mean(R₂, dims = 2), label = \"epsilon = 0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "##  Simple bandit algorithm\n",
    "\n",
    "1. Please implement the $\\epsilon$-greedy algorithm with a constant step-size $\\alpha$, as described in the equation (2.5).\n",
    "2. Run 2000 problems with 1000 steps in each.\n",
    "   Compare the following $\\epsilon$ settings:\n",
    "  - $\\epsilon = 0.01$\n",
    "  - $\\epsilon = 0.1$\n",
    "  - $\\epsilon = 0.2$\n",
    " \n",
    " Please plot the result.\n",
    "3. Run 2000 problems with 5000 steps in each and plot the result.\n",
    "\n",
    "4. Adjust the standard diviation of `MultiArmedBandit`\n",
    "```\n",
    "MultiArmedBandit(10, 1000, σ = 5)\n",
    "```\n",
    "  Then, re-run the setting described in 2. and 3.\n",
    "  \n",
    "5. Compare the results between low and high variance. That is,\n",
    "   $\\sigma = 1$ vs $\\sigma = 5$. Describe your finding.\n",
    "\n",
    "## Optimistic Inital Values\n",
    "\n",
    "1. Please implmenet the algorithm **greedy with optimistic initial values** described in page 34.\n",
    "2. Plot the result of different initial values:\n",
    "  - $Q_1 = 1$\n",
    "  - $Q_1 = 5$\n",
    "\n",
    "## Upper-Confidence-Bound Action Selection\n",
    "\n",
    "1. Please implement the UCB algorithm describe in equation (2.10) page 35.\n",
    "   Consider following setting of $c$:\n",
    "   - $c = 1$\n",
    "   - $c = 2$\n",
    "   - $c = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
